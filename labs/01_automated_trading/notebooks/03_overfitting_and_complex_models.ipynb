{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Overfitting & Complex Models\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this session, we will:\n",
    "\n",
    "1. **3-Class Classification** - Predict positive, negative, and flat returns\n",
    "2. **Explore multiple models** - From simple to complex (Logistic Regression → Random Forest → Gradient Boosting → Neural Networks)\n",
    "3. **Understand overfitting** - See how complex models can memorize training data but fail on test data\n",
    "4. **Visualize model performance** - Interactive plots with confusion matrices, learning curves, and metrics\n",
    "5. **Analyze bias-variance trade-off** - When to use simple vs complex models\n",
    "\n",
    "## The Problem\n",
    "\n",
    "In Sessions 1-2, we used binary classification (up/down). But in reality, many days have **flat returns** (near zero). \n",
    "\n",
    "**3-Class Classification:**\n",
    "- **Positive**: Return > threshold (e.g., > 0.001)\n",
    "- **Flat**: -threshold ≤ Return ≤ threshold (e.g., -0.001 to 0.001)\n",
    "- **Negative**: Return < -threshold (e.g., < -0.001)\n",
    "\n",
    "**Key question:** Can complex models improve performance, or do they just overfit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Plotly for Jupyter notebooks\n",
    "# Set default renderer - works in Jupyter Notebook, JupyterLab, and VS Code\n",
    "pio.renderers.default = \"notebook\"  # Use \"jupyterlab\" if in JupyterLab, \"vscode\" if in VS Code\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Import our modules\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "from eda.analysis import basic_summary\n",
    "from features.engineering import prepare_features, prepare_target\n",
    "from backtesting.engine import backtest_strategy, print_backtest_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Session 2 data (same dataset with complex non-linear relationship)\n",
    "data_path = Path(\"../data/saved/stock_session2.csv\")\n",
    "df = pd.read_csv(data_path, parse_dates=[\"timestamp\"])\n",
    "\n",
    "print(f\"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare 3-Class Target\n",
    "\n",
    "We'll create three classes based on a threshold parameter. This threshold defines what we consider \"flat\" returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETER: Threshold for flat returns (students can play with this)\n",
    "FLAT_THRESHOLD = 0.003  # Returns between -0.003 and 0.003 are considered \"flat\"\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [\"X1\", \"X2\", \"X3\", \"X4\"]\n",
    "X = prepare_features(df, feature_cols=feature_cols)\n",
    "\n",
    "# Prepare target: 3-class classification\n",
    "y_returns = prepare_target(df, target_col=\"returns\")\n",
    "\n",
    "# Create 3 classes: -1 (negative), 0 (flat), 1 (positive)\n",
    "# TODO fill the blank to create a multi-class label for y_3_class, using the FLAT_THRESHOLD\n",
    "y_3class = pd.Series(\n",
    "    ...\n",
    ")\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"\\n3-Class Distribution:\")\n",
    "print(y_3class.value_counts().sort_index())\n",
    "print(f\"\\nClass percentages:\")\n",
    "for cls in [-1, 0, 1]:\n",
    "    pct = (y_3class == cls).mean() * 100\n",
    "    label = {-1: \"Negative\", 0: \"Flat\", 1: \"Positive\"}[cls]\n",
    "    print(f\"  {label}: {pct:.2f}%\")\n",
    "\n",
    "# Visualize return distribution with thresholds\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(y_returns, bins=100, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(FLAT_THRESHOLD, color='r', linestyle='--', label=f'Positive threshold: {FLAT_THRESHOLD}')\n",
    "plt.axvline(-FLAT_THRESHOLD, color='r', linestyle='--', label=f'Negative threshold: {-FLAT_THRESHOLD}')\n",
    "plt.axvline(0, color='gray', linestyle='-', alpha=0.5)\n",
    "plt.xlabel('Returns')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Return Distribution with 3-Class Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split\n",
    "\n",
    "**Important:** We'll use the same split as before. The time-varying component (X4 in second half) will help us see overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronological split: 80% train, 20% test\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y_3class.iloc[:split_idx]\n",
    "y_test = y_3class.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts().sort_index())\n",
    "\n",
    "# Check time-varying component: X4 only affects second half\n",
    "print(f\"\\nTime-varying component check:\")\n",
    "print(f\"  First half (train): X4 mean = {X_train['X4'].mean():.4f}\")\n",
    "print(f\"  Second half (test): X4 mean = {X_test['X4'].mean():.4f}\")\n",
    "print(f\"  Note: X4 relationship only active in second half - models may overfit to first half patterns!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiple Models: Simple to Complex\n",
    "\n",
    "We'll test models of increasing complexity and see how they perform on train vs test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Define models to test (simple to complex)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree (depth=5)': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Decision Tree (depth=20)': DecisionTreeClassifier(max_depth=20, random_state=42),\n",
    "    'Random Forest (10 trees)': RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42),\n",
    "    'Random Forest (100 trees)': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'Neural Network (small)': MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42),\n",
    "    'Neural Network (large)': MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=1000, random_state=42),\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    # TODO fill the blanks\n",
    "    model.fit(...)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = \n",
    "    y_test_pred = \n",
    "    \n",
    "    # Metrics\n",
    "    train_acc = \n",
    "    test_acc = \n",
    "    train_f1 = \n",
    "    test_f1 = \n",
    "    \n",
    "    # Gap (overfitting indicator)\n",
    "    acc_gap = \n",
    "    f1_gap = \n",
    "    \n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'train_f1': train_f1,\n",
    "        'test_f1': test_f1,\n",
    "        'acc_gap': acc_gap,\n",
    "        'f1_gap': f1_gap,\n",
    "        'model_obj': model,\n",
    "        'y_test_pred': y_test_pred\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df[['model', 'train_acc', 'test_acc', 'acc_gap', 'train_f1', 'test_f1', 'f1_gap']].round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overfitting: Train vs Test Accuracy\n",
    "\n",
    "# TODO do a visualization for train vs test for all models, showcasing the overfitting gap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Overfitting Patterns\n",
    "\n",
    "**What patterns do you see?**\n",
    "\n",
    "\n",
    "**Key insight:** More complexity ≠ Better performance. The sweet spot balances bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices\n",
    "\n",
    "Let's visualize confusion matrices for different models to see where they make mistakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few models to visualize\n",
    "models_to_plot = ['Logistic Regression', 'Decision Tree (depth=20)', 'Random Forest (100 trees)', 'Neural Network (large)']\n",
    "\n",
    "# TODO for each of the above models, visualize the confusion matrices (NB you can plot them using plotly heatmaps to visualize things better)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1: Confusion Matrix Interpretation\n",
    "\n",
    "**What do the confusion matrices tell us?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-Class Performance Metrics\n",
    "\n",
    "Let's see how each model performs on each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics for each model\n",
    "per_class_results = []\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    y_pred = row['y_test_pred']\n",
    "    \n",
    "    # Per-class precision, recall, F1\n",
    "    precision = precision_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "    \n",
    "    for i, cls in enumerate([-1, 0, 1]):\n",
    "        per_class_results.append({\n",
    "            'model': row['model'],\n",
    "            'class': {-1: 'Negative', 0: 'Flat', 1: 'Positive'}[cls],\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1': f1[i]\n",
    "        })\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_results)\n",
    "\n",
    "# Interactive plot: Per-class F1 scores\n",
    "fig = go.Figure()\n",
    "\n",
    "for cls in ['Negative', 'Flat', 'Positive']:\n",
    "    cls_data = per_class_df[per_class_df['class'] == cls]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=cls_data['model'],\n",
    "        y=cls_data['f1'],\n",
    "        mode='lines+markers',\n",
    "        name=cls,\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Per-Class F1 Score by Model',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='F1 Score',\n",
    "    hovermode='x unified',\n",
    "    height=500,\n",
    "    xaxis=dict(tickangle=-45)\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"notebook\")\n",
    "\n",
    "# Heatmap of F1 scores\n",
    "pivot_f1 = per_class_df.pivot(index='model', columns='class', values='f1')\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=pivot_f1.values,\n",
    "    x=pivot_f1.columns,\n",
    "    y=pivot_f1.index,\n",
    "    colorscale='Viridis',\n",
    "    text=pivot_f1.values.round(3),\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 10},\n",
    "    colorbar=dict(title=\"F1 Score\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='F1 Score Heatmap: Model vs Class',\n",
    "    xaxis_title='Class',\n",
    "    yaxis_title='Model',\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Select a few models for learning curves\n",
    "# TODO use learning_curve to see how fast each model learns (only for logistic and tree models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis\n",
    "\n",
    "Let's see which features different models consider important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use, when available, model.feature_importances_ to analyze feature importances per models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.1: Feature Importance Insights\n",
    "\n",
    "**What can we learn from feature importance?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary: Bias-Variance Trade-off\n",
    "\n",
    "Let's create a final visualization showing the bias-variance trade-off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO plot for each model train accuracy, test accuracy and overfitting gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
