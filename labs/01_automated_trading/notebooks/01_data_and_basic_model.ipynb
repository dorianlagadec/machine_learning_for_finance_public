{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Data Analysis & Basic Model\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this session, we will:\n",
    "\n",
    "1. **Load and explore financial data** - Understand the structure, quality, and characteristics of our dataset\n",
    "2. **Perform exploratory data analysis (EDA)** - Identify patterns, outliers, missing data, and relationships\n",
    "3. **Build a simple linear regression model** - Predict returns using features\n",
    "4. **Implement a basic trading strategy** - Convert predictions into buy/sell signals\n",
    "5. **Backtest the strategy** - Evaluate performance using both ML metrics (RMSE, R²) and financial metrics (Sharpe ratio, drawdown)\n",
    "6. **Identify data leakage** - Understand one of the most common pitfalls in financial ML\n",
    "\n",
    "## The Problem\n",
    "\n",
    "We want to build a **systematic trading strategy** that:\n",
    "- Uses machine learning to predict future returns\n",
    "- Makes trading decisions **without any arbitrary rules** (pure model-based)\n",
    "- Can be backtested and evaluated objectively\n",
    "\n",
    "The key question: **Can we predict $r_{t+1}$ (return from time $t$ to $t+1$) using features observed at time $t$?**\n",
    "\n",
    "$$r_{t+1} = \\frac{P_{t+1} - P_t}{P_t}$$\n",
    "\n",
    "Where $P_t$ is the price at time $t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Import our modules - add parent directory to path\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "from eda.analysis import basic_summary\n",
    "from features.engineering import prepare_features, prepare_target\n",
    "from backtesting.engine import backtest_strategy, print_backtest_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path(\"../data/saved/stock_a.csv\")\n",
    "df = pd.read_csv(data_path, parse_dates=[\"timestamp\"])\n",
    "\n",
    "print(f\"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Data Structure\n",
    "\n",
    "**What columns do we have? What does each represent?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic summary\n",
    "basic_summary(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Missing Data\n",
    "\n",
    "**How much missing data do we have? In which columns?**\n",
    "\n",
    "\n",
    "**Important:** We should NOT have missing data in `returns` or `price` - these are critical for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data\n",
    "\n",
    "# TODO plot or calculate the proportion of missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Price and Returns Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot price series\n",
    "\n",
    "# TODO plot prices and returns Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: Returns Distribution\n",
    "\n",
    "**What does the distribution of returns look like? Is it normal?**\n",
    "\n",
    "\n",
    "Let's check our synthetic data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns distribution\n",
    "\n",
    "# Plot returns distributions and show some statistics around it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "feature_cols = [\"X1\", \"X2\", \"X3\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, col in enumerate(feature_cols):\n",
    "    df[col].hist(bins=50, ax=axes[i], edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"{col} Distribution\")\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Feature-Target Relationship\n",
    "\n",
    "**Do our features have predictive power? Let's check correlations.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "# TODO analyze univariate relationships between features and returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Note on Automated EDA Tools\n",
    "\n",
    "**ydata-profiling** (formerly pandas-profiling) is a useful Python package that automatically generates comprehensive HTML reports with detailed statistics, correlations, missing data analysis, and visualizations. It can save significant time during exploratory data analysis.\n",
    "\n",
    "To use it:\n",
    "```python\n",
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title=\"Data Profile\")\n",
    "profile.to_file(\"report.html\")\n",
    "```\n",
    "\n",
    "For this lab, we'll use manual EDA to understand each step, but in practice, automated tools like ydata-profiling can be very helpful for quick data overviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "For now, feature engineering is minimal - we'll just handle missing values. In future sessions, we'll add:\n",
    "- Technical indicators (moving averages, RSI, etc.)\n",
    "- Lagged features\n",
    "- Rolling statistics\n",
    "- Feature interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "\n",
    "# TODO look into the imported functions and fill the todos\n",
    "\n",
    "X = prepare_features(df, feature_cols=feature_cols)\n",
    "y = prepare_target(df, target_col=\"returns\")\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nMissing values after processing: {X.isnull().sum().sum()}\")\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n",
    "\n",
    "**Critical for time series:** We must use a **chronological split**, not random!\n",
    "\n",
    "Why? In real trading, we can't use future data to predict the past. Random splits would give us unrealistic performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronological split: 80% train, 20% test\n",
    "\n",
    "# TODO split X, y and df with a chronological 80-20% split\n",
    "\n",
    "X_train =\n",
    "X_test = \n",
    "y_train =\n",
    "y_test =\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples ({X_train.index[0]} to {X_train.index[-1]})\")\n",
    "print(f\"Test set: {len(X_test)} samples ({X_test.index[0]} to {X_test.index[-1]})\")\n",
    "\n",
    "# Also split the full dataframe for backtesting\n",
    "df_train = \n",
    "df_test = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear Regression Model\n",
    "\n",
    "We'll start with the simplest model: **Linear Regression**\n",
    "\n",
    "$$\\hat{r}_{t+1} = \\beta_0 + \\beta_1 X_{1,t} + \\beta_2 X_{2,t} + \\beta_3 X_{3,t} + \\epsilon_t$$\n",
    "\n",
    "Where $\\hat{r}_{t+1}$ is the predicted return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Model coefficients\n",
    "print(\"Model Coefficients:\")\n",
    "for i, col in enumerate(X.columns):\n",
    "    print(f\"  {col}: {model.coef_[i]:.6f}\")\n",
    "print(f\"  Intercept: {model.intercept_:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Evaluation (ML Metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ML metrics\n",
    "train_rmse = \n",
    "test_rmse = \n",
    "train_mae =\n",
    "test_mae = \n",
    "train_r2 =\n",
    "test_r2 = \n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE (ML METRICS)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTrain Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.6f}\")\n",
    "print(f\"  MAE: {train_mae:.6f}\")\n",
    "print(f\"  R²: {train_r2:.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.6f}\")\n",
    "print(f\"  MAE: {test_mae:.6f}\")\n",
    "print(f\"  R²: {test_r2:.4f}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1: Model Interpretation\n",
    "\n",
    "**What do these metrics tell us?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "\n",
    "# TODO implement visualization of predicted returns vs actual returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trading Strategy Implementation\n",
    "\n",
    "**Simple strategy:**\n",
    "- If predicted return > 0: **BUY** (go long)\n",
    "- If predicted return < 0: **SELL** (go short)\n",
    "\n",
    "**Assumptions (for now):**\n",
    "- We can trade at the **close price** (we'll relax this later)\n",
    "- Transaction costs: 0.1% per trade\n",
    "- We can go both long and short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to test dataframe\n",
    "df_test[\"prediction\"] = y_test_pred\n",
    "\n",
    "# Simple strategy: buy if prediction > 0, sell if prediction < 0\n",
    "df_test[\"signal\"] = np.where(df_test[\"prediction\"] > 0, 1, -1)\n",
    "\n",
    "print(\"Signal distribution:\")\n",
    "print(df_test[\"signal\"].value_counts())\n",
    "print(f\"\\n% Long: {(df_test['signal'] == 1).sum() / len(df_test) * 100:.2f}%\")\n",
    "print(f\"% Short: {(df_test['signal'] == -1).sum() / len(df_test) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Backtesting\n",
    "\n",
    "Now let's see if our strategy is actually profitable!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest\n",
    "results = backtest_strategy(\n",
    "    df_test,\n",
    "    y_test_pred,\n",
    "    initial_capital=100000,\n",
    "    transaction_cost=0.001,  # 0.1%\n",
    "    trade_at=\"close\"\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print_backtest_metrics(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7.1: Interpreting Backtest Results\n",
    "\n",
    "**What do these metrics mean?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot equity curve\n",
    "\n",
    "# TODO plot the equity curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 8. Exercise: Find the Data Leakage Bug!\n",
    "\n",
    "**Challenge:** There's a subtle data leakage issue in our current setup. Can you find it?\n",
    "\n",
    "**Hint:** Think about when we observe features vs when we can actually trade.\n",
    "\n",
    "\n",
    "**Challenge:** Re-run the same analysis with `stock_b` that has been generated with little auto-correlation in returns. Do you see a difference? How can you explain it?\n",
    "\n",
    "**Hint:** Now we have little but real explanatory power on the returns at $t+2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this session, we:\n",
    "\n",
    "1. ✅ Loaded and explored financial data\n",
    "2. ✅ Performed EDA to understand data characteristics\n",
    "3. ✅ Built a linear regression model to predict returns\n",
    "4. ✅ Implemented a simple trading strategy\n",
    "5. ✅ Backtested the strategy with both ML and financial metrics\n",
    "6. ✅ Identified a data leakage issue (latency between observation and execution)\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Good ML metrics ≠ Profitable trading\n",
    "- Chronological train-test split is essential\n",
    "- Transaction costs matter!\n",
    "- Latency/execution timing is critical\n",
    "\n",
    "**Next Session:** We'll explore logistic regression for direction prediction, threshold tuning, and more sophisticated backtesting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
